{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f095bd",
   "metadata": {},
   "source": [
    "# Detect text reuse between DNZ and MEGA texts using Passim\n",
    "\n",
    "- **Input**: DNZ texts from `texts/DNZ_texts/` and MEGA texts from `texts/MEGA_texts/`\n",
    "- **Output**: `passim_output`\n",
    "\n",
    "\n",
    "Note:\n",
    "- **MEGA corpus** as the **source corpus** (the original texts being cited)\n",
    "- **DNZ corpus** as the **target corpus** (the texts that cite other works)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "222ba425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/ht8933/Documents/dev/remarx\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterator, Optional\n",
    "\n",
    "import ftfy\n",
    "import orjsonl\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set working directory to project root\n",
    "project_root = Path.cwd()\n",
    "print(f\"Working directory: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87af7bcf",
   "metadata": {},
   "source": [
    "## 1. Convert `text` files to `Passim`-friendly `JSONL` format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2bdea",
   "metadata": {},
   "source": [
    "### Step 1: Convert `text` files to raw `JSONL` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2bae019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output written to: /Users/ht8933/Documents/dev/remarx/texts_json_for_passim/dnz_texts.jsonl\n",
      "Output written to: /Users/ht8933/Documents/dev/remarx/texts_json_for_passim/mega_texts.jsonl\n",
      "DNZ/MEGA texts have been converted to JSONL and saved in texts_json_for_passim/\n",
      "All JSONL files are located in: /Users/ht8933/Documents/dev/remarx/texts_json_for_passim\n"
     ]
    }
   ],
   "source": [
    "def convert_text_to_jsonl(text_dir, output_file, series_name):\n",
    "    text_dir = Path(text_dir)\n",
    "    output_file = Path(output_file)\n",
    "    count = 0\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for txt_file in text_dir.glob(\"*.txt\"):\n",
    "            try:\n",
    "                with open(txt_file, \"r\", encoding=\"utf-8\") as txt_f:\n",
    "                    content = txt_f.read().strip()\n",
    "                doc = {\"id\": txt_file.stem, \"text\": content, \"series\": series_name}\n",
    "                f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
    "                count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {txt_file}: {e}\")\n",
    "    print(f\"Output written to: {output_file.resolve()}\")\n",
    "    return count\n",
    "\n",
    "\n",
    "# DNZ / MEGA original JSONL\n",
    "texts_json_for_passim_dir = Path(\"texts_json_for_passim\")\n",
    "if texts_json_for_passim_dir.exists():\n",
    "    shutil.rmtree(texts_json_for_passim_dir)\n",
    "texts_json_for_passim_dir.mkdir(exist_ok=True)\n",
    "\n",
    "convert_text_to_jsonl(\n",
    "    \"texts/DNZ_texts\", texts_json_for_passim_dir / \"dnz_texts.jsonl\", \"dnz\"\n",
    ")\n",
    "convert_text_to_jsonl(\n",
    "    \"texts/MEGA_texts\", texts_json_for_passim_dir / \"mega_texts.jsonl\", \"mega\"\n",
    ")\n",
    "\n",
    "print(\"DNZ/MEGA texts have been converted to JSONL and saved in texts_json_for_passim/\")\n",
    "print(f\"All JSONL files are located in: {texts_json_for_passim_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92642992",
   "metadata": {},
   "source": [
    "### Step 2: Convert raw `JSONL` to Passim-friendly `JSONL` using the code adapted from `corppa`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc39df51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming records: 24it [00:13,  1.73it/s]\n",
      "Transforming records: 642it [00:00, 1626.55it/s]\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean text to make finding matches easier.\"\"\"\n",
    "    result_text = ftfy.fix_text(\n",
    "        text,\n",
    "        unescape_html=False,\n",
    "        fix_encoding=False,\n",
    "        normalization=\"NFKC\",\n",
    "    )\n",
    "    result_text = re.sub(r\"\\s+\", \" \", result_text)\n",
    "    return result_text\n",
    "\n",
    "\n",
    "def transform_record(\n",
    "    record: Dict[str, Any],\n",
    "    corpus_name: str,\n",
    "    id_field: str = \"id\",\n",
    "    preserve_fields: bool = False,\n",
    "    corpus_from_field: Optional[str] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convert one record to passim-friendly dict:\n",
    "      - id: taken from record[id_field]\n",
    "      - text: cleaned text (missing -> \"\")\n",
    "    \"\"\"\n",
    "    if id_field not in record:\n",
    "        raise ValueError(f\"Record missing required id_field '{id_field}'\")\n",
    "\n",
    "    out_record: Dict[str, Any] = {}\n",
    "    if preserve_fields:\n",
    "        if id_field != \"id\" and \"id\" in record:\n",
    "            raise ValueError(\"Record already has 'id' while id_field != 'id'\")\n",
    "        out_record.update(record)\n",
    "\n",
    "    out_record[\"id\"] = record[id_field]\n",
    "    if corpus_from_field:\n",
    "        if corpus_from_field not in record:\n",
    "            raise ValueError(f\"Record missing corpus_from_field '{corpus_from_field}'\")\n",
    "        pass\n",
    "    else:\n",
    "        pass  # out_record[\"corpus\"] = corpus_name\n",
    "\n",
    "    out_record[\"text\"] = clean_text(record.get(\"text\", \"\"))\n",
    "    return out_record\n",
    "\n",
    "\n",
    "def build_passim_corpus(\n",
    "    input_corpus: Path,\n",
    "    id_field: str = \"id\",\n",
    "    preserve_fields: bool = False,\n",
    "    corpus_from_field: Optional[str] = None,\n",
    "    show_progress: bool = True,\n",
    ") -> Iterator[Dict[str, Any]]:\n",
    "    \"\"\"Generator over transformed records\"\"\"\n",
    "    for record in tqdm(\n",
    "        orjsonl.stream(input_corpus),\n",
    "        desc=\"Transforming records\",\n",
    "        disable=not show_progress,\n",
    "    ):\n",
    "        yield transform_record(\n",
    "            record,\n",
    "            corpus_name=\"\",\n",
    "            id_field=id_field,\n",
    "            preserve_fields=preserve_fields,\n",
    "            corpus_from_field=corpus_from_field,\n",
    "        )\n",
    "\n",
    "\n",
    "def save_passim_corpus(\n",
    "    input_corpus: Path,\n",
    "    output_corpus: Path,\n",
    "    id_field: str = \"id\",\n",
    "    preserve_fields: bool = True,\n",
    "    corpus_from_field: Optional[str] = None,\n",
    "    show_progress: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"Transform and write to JSONL”\"\"\"\n",
    "    output_corpus.parent.mkdir(parents=True, exist_ok=True)\n",
    "    orjsonl.save(\n",
    "        output_corpus,\n",
    "        build_passim_corpus(\n",
    "            input_corpus=input_corpus,\n",
    "            id_field=id_field,\n",
    "            preserve_fields=preserve_fields,\n",
    "            corpus_from_field=corpus_from_field,\n",
    "            show_progress=show_progress,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "dnz_jsonl = texts_json_for_passim_dir / \"dnz_texts.jsonl\"\n",
    "mega_jsonl = texts_json_for_passim_dir / \"mega_texts.jsonl\"\n",
    "dnz_passim = texts_json_for_passim_dir / \"dnz_passim.jsonl\"\n",
    "mega_passim = texts_json_for_passim_dir / \"mega_passim.jsonl\"\n",
    "\n",
    "save_passim_corpus(\n",
    "    input_corpus=dnz_jsonl,\n",
    "    output_corpus=dnz_passim,\n",
    "    id_field=\"id\",\n",
    "    preserve_fields=True,\n",
    "    corpus_from_field=None,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "save_passim_corpus(\n",
    "    input_corpus=mega_jsonl,\n",
    "    output_corpus=mega_passim,\n",
    "    id_field=\"id\",\n",
    "    preserve_fields=True,\n",
    "    corpus_from_field=None,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e7570b",
   "metadata": {},
   "source": [
    "### Step 3: Combine the two passim input files into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2e19ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Passim input created: texts_json_for_passim/combined_passim_input.jsonl\n",
      "File size: 73.83 MB\n",
      "Order: MEGA texts (source) first, then DNZ texts (target)\n"
     ]
    }
   ],
   "source": [
    "combined_passim_input = texts_json_for_passim_dir / \"combined_passim_input.jsonl\"\n",
    "with open(combined_passim_input, \"w\", encoding=\"utf-8\") as outf:\n",
    "    for p in (mega_passim, dnz_passim):\n",
    "        with open(p, \"r\", encoding=\"utf-8\") as inf:\n",
    "            outf.write(inf.read())\n",
    "\n",
    "print(f\"Combined Passim input created: {combined_passim_input}\")\n",
    "print(f\"File size: {combined_passim_input.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "print(\"Order: MEGA texts (source) first, then DNZ texts (target)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3605f7",
   "metadata": {},
   "source": [
    "## 2. Run `Passim` on converted `JSONL` corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa27ad",
   "metadata": {},
   "source": [
    "### Step 1: In order to run `Passim`, we need to first configure `java_home` environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24740508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to your own java_home\n",
    "java_home = \"/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\"\n",
    "\n",
    "# If you do not know your `java_home` path, run the following helper function below (if may not work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5758f8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME = /opt/homebrew/Cellar/openjdk@17/17.0.16/libexec/openjdk.jdk/Contents/Home\n",
      "openjdk version \"17.0.16\" 2025-07-15\n"
     ]
    }
   ],
   "source": [
    "# Helper function to find JAVA_HOME\n",
    "\n",
    "\n",
    "def find_java_home(prefer_version: str = \"17\") -> str | None:\n",
    "    try:\n",
    "        out = subprocess.check_output(\n",
    "            [\"/usr/libexec/java_home\", \"-v\", prefer_version], text=True\n",
    "        ).strip()\n",
    "        if out and Path(out, \"bin/java\").exists():\n",
    "            return out\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Common installation paths (Homebrew / Temurin / Zulu)\n",
    "    candidates = [\n",
    "        \"/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\",  # brew (Apple Silicon)\n",
    "        \"/usr/local/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\",  # brew (Intel)\n",
    "        \"/Library/Java/JavaVirtualMachines/temurin-17.jdk/Contents/Home\",  # Temurin\n",
    "        \"/Library/Java/JavaVirtualMachines/zulu-17.jdk/Contents/Home\",  # Azul Zulu\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if Path(p, \"bin/java\").exists():\n",
    "            return p\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "java_home = find_java_home(\"17\")\n",
    "if not java_home:\n",
    "    raise RuntimeError(\"Failed to find Java 17 installation.\\n\")\n",
    "\n",
    "# set java_home to environment\n",
    "os.environ[\"JAVA_HOME\"] = java_home\n",
    "os.environ[\"PATH\"] = f\"{Path(java_home) / 'bin'}:{os.environ.get('PATH', '')}\"\n",
    "\n",
    "# log java version\n",
    "ver = subprocess.check_output(\n",
    "    [Path(java_home, \"bin/java\"), \"-version\"], stderr=subprocess.STDOUT, text=True\n",
    ")\n",
    "print(\"JAVA_HOME =\", java_home)\n",
    "print(ver.splitlines()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2465f05",
   "metadata": {},
   "source": [
    "### Step 2: Configure other stuff for running `Passim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9e223ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Spark Configuration:\n",
      "  spark.sql.shuffle.partitions: 200\n",
      "  spark.driver.maxResultSize: 2g\n",
      "  spark.sql.adaptive.enabled: true\n",
      "  spark.sql.adaptive.coalescePartitions.enabled: true\n",
      "  spark.serializer: org.apache.spark.serializer.KryoSerializer\n",
      "  spark.sql.debug.maxToStringFields: 1000\n",
      "  spark.sql.execution.arrow.pyspark.enabled: false\n",
      "\n",
      "Passim Configuration:\n",
      "  Driver Memory: 8g\n",
      "  Executor Memory: 8g\n",
      "  Min Document Frequency: 2\n",
      "  Min Match Length: 5\n",
      "  N-gram Size: 3\n"
     ]
    }
   ],
   "source": [
    "# Memory settings\n",
    "driver_memory = \"8g\"\n",
    "executor_memory = \"8g\"\n",
    "\n",
    "# Spark optimization parameters\n",
    "spark_configs = {\n",
    "    \"spark.sql.shuffle.partitions\": \"200\",  # 50\n",
    "    \"spark.driver.maxResultSize\": \"2g\",\n",
    "    \"spark.sql.adaptive.enabled\": \"true\",\n",
    "    \"spark.sql.adaptive.coalescePartitions.enabled\": \"true\",\n",
    "    \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "    \"spark.sql.debug.maxToStringFields\": \"1000\",\n",
    "    \"spark.sql.execution.arrow.pyspark.enabled\": \"false\",  # Disable Arrow for stability\n",
    "}\n",
    "\n",
    "# Passim algorithm parameters\n",
    "min_df = 2  # Minimum document frequency (filter rare words)\n",
    "min_match = 5  # Minimum match length\n",
    "ngram_size = 3  # N-gram size\n",
    "\n",
    "# Environment variables\n",
    "env_vars = {\n",
    "    \"SPARK_LOCAL_IP\": \"127.0.0.1\",\n",
    "    \"PYSPARK_DRIVER_PYTHON\": \"python3\",\n",
    "    \"PYSPARK_PYTHON\": \"python3\",\n",
    "}\n",
    "\n",
    "# Generate Passim arguments\n",
    "passim_args = [\"--minDF\", str(min_df), \"-m\", str(min_match), \"-n\", str(ngram_size)]\n",
    "\n",
    "print(\"Updated Spark Configuration:\")\n",
    "for key, value in spark_configs.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "\n",
    "print(\"\\nPassim Configuration:\")\n",
    "print(f\"  Driver Memory: {driver_memory}\")\n",
    "print(f\"  Executor Memory: {executor_memory}\")\n",
    "print(f\"  Min Document Frequency: {min_df}\")\n",
    "print(f\"  Min Match Length: {min_match}\")\n",
    "print(f\"  N-gram Size: {ngram_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42382a53",
   "metadata": {},
   "source": [
    "### Step 3: Run `Passim` on converted corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dda6ff24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Passim on DNZ and MEGA texts:\n",
      "==================================================\n",
      "Command: passim /Users/ht8933/Documents/dev/remarx/texts_json_for_passim/combined_passim_input.jsonl /Users/ht8933/Documents/dev/remarx/passim_output\n",
      "\n",
      "⏱️ Starting... (this may take a few minutes)\n",
      "✅ Passim completed successfully!\n",
      "\n",
      " Success! Now check the /Users/ht8933/Documents/dev/remarx/passim_output directory\n"
     ]
    }
   ],
   "source": [
    "combined_input = Path(\n",
    "    \"/Users/ht8933/Documents/dev/remarx/texts_json_for_passim/combined_passim_input.jsonl\"\n",
    ")\n",
    "output_dir = Path(\"/Users/ht8933/Documents/dev/remarx/passim_output\")\n",
    "\n",
    "# Remove the output directory if it exists\n",
    "if output_dir.exists():\n",
    "    shutil.rmtree(output_dir)\n",
    "\n",
    "cmd = [\"passim\", str(combined_input), str(output_dir)]\n",
    "\n",
    "# use filterpairs to ensure MEGA (source) → DNZ (target) direction\n",
    "# cmd = [\n",
    "#     \"passim\",\n",
    "#     str(combined_input),\n",
    "#     str(output_dir),\n",
    "#     '--filterpairs', 'corpus = \"mega\" AND corpus2 = \"dnz\"'\n",
    "# ]\n",
    "\n",
    "print(\"Running Passim on DNZ and MEGA texts:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Command: {' '.join(cmd)}\")\n",
    "print(\"\\n⏱️ Starting... (this may take a few minutes)\")\n",
    "\n",
    "# Set environment variables\n",
    "env = os.environ.copy()\n",
    "env.update(\n",
    "    {\n",
    "        \"SPARK_LOCAL_IP\": \"127.0.0.1\",\n",
    "        \"PYSPARK_DRIVER_PYTHON\": \"python3\",  # use system python, not conda python\n",
    "        \"PYSPARK_PYTHON\": \"python3\",\n",
    "        \"SPARK_DRIVER_MEMORY\": \"8g\",\n",
    "        \"SPARK_EXECUTOR_MEMORY\": \"8g\",\n",
    "    }\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        cmd, cwd=project_root, env=env, capture_output=True, text=True\n",
    "    )\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(\"✅ Passim completed successfully!\")\n",
    "        success_default = True\n",
    "    else:\n",
    "        print(f\"❌ Passim failed, return code: {result.returncode}\")\n",
    "        print(\"Error message:\", result.stderr)\n",
    "        success_default = False\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error running Passim: {e}\")\n",
    "    success_default = False\n",
    "\n",
    "if success_default:\n",
    "    print(f\"\\n Success! Now check the {output_dir} directory\")\n",
    "else:\n",
    "    print(\"\\n Failed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
